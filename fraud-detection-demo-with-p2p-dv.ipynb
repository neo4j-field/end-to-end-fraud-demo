{"cells": [{"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "# Exploring Fraud Detection With Neo4j & Graph Data Science\n\nThis analysis uses [Neo4j and Graph Data Science (GDS)](https://neo4j.com/docs/graph-data-science/current/) to explore an anonymized data sample from a Peer-to-Peer (P2P) payment platform.  The notebook is split up into the following sections to cover various stages of the graph data science workflow:\n\n- Notebook Setup\n- Part 1: Exploring Connected Fraud Data\n- Part 2: Resolving Fraud Communities using Entity Resolution and Community Detection\n- Part 3: Recommending Suspicious Accounts With Centrality & Node Similarity\n- Part 4: Predicting Fraud Risk Accounts with Machine Learning"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Notebook Setup <a name=\"p0\"></a>"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install -U graphdatascience"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from neo4j import GraphDatabase\nimport pandas as pd\nimport configparser\nimport os\npd.set_option('display.width', 0)\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 50)"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Neo4j Settings\nThe `NEO4J_PROPERTIES_FILE` is an ini configuration file for Neo4j properties so this notebook can connect to\nyour Neo4j instance and load data. The ini file should be formatted as follows\n```\n[NEO4J]\nPASSWORD=<password>\nUSERNAME=<database name, is 'neo4j' by default>\nHOST=<host uri>\n```\n\nSet `NEO4J_PROPERTIES_FILE` to None or any non-existent location to use the below defaults\n```\nHOST = 'neo4j://localhost'\nUSERNAME = 'neo4j'\nPASSWORD = 'password'\n```"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "NEO4J_PROPERTIES_FILE=\"gs://neo4j_voutila/neo4j.ini\""}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from google.cloud import storage\nfrom google.cloud.storage.blob import Blob\n\ngcs = storage.Client()\nini = Blob.from_string(NEO4J_PROPERTIES_FILE, client=gcs)\nassert ini.exists()\n\nHOST, USERNAME, PASSWORD = None, \"neo4j\", \"password\"\ntry:\n    config = configparser.RawConfigParser()\n    text = ini.download_as_text()\n    config.read_string(text)\n    HOST = config['NEO4J']['HOST']\n    USERNAME = config['NEO4J']['USERNAME']\n    PASSWORD = config['NEO4J']['PASSWORD']\n    print(f'Using custom database properties: {USERNAME}@{HOST}')\nexcept Exception as e:\n    print(e)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "driver = GraphDatabase.driver(HOST, auth=(USERNAME, PASSWORD))\ndriver.verify_connectivity()"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Helper Functions"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "def run(driver, query, params={}):\n    with driver.session() as session:\n        if params:\n            return [r for r in session.run(query, params)]\n        return [r for r in session.run(query)]\n    \ndef clear_graph(driver, graph_name):\n    if run(driver, f\"CALL gds.graph.exists('{graph_name}') YIELD exists RETURN exists\")[0].get(\"exists\"):\n        run(driver, f\"CALL gds.graph.drop('{graph_name}')\")\n\ndef clear_all_graphs(driver):\n    graphs = run(driver, 'CALL gds.graph.list() YIELD graphName RETURN collect(graphName) as graphs')[0].get('graphs')\n    for g in graphs:\n        run(driver, f\"CALL gds.graph.drop('{g}')\")\n\ndef identifier_degrees(user_label, degree_property):\n    params = {'userLabel': user_label, 'property': degree_property}\n    clear_graph(driver, 'id-projection')\n    print(f'projecting identity graph for {user_label}')\n    run(driver, '''\n        CALL gds.graph.project('id-projection',\n            [$userLabel, 'Card', 'Device', 'IP'],\n            {\n                HAS_CC: {orientation: 'REVERSE'},\n                HAS_IP: {orientation: 'REVERSE'},\n                USED: {orientation: 'REVERSE'}\n            }\n        )\n    ''', params=params)\n\n    print(f'calculating degree centrality')\n    run(driver, '''\n        CALL gds.degree.mutate('id-projection', {\n            mutateProperty: $property\n        })\n    ''', params=params)\n\n    print(f'writing degree scores to {degree_property}')\n    run(driver, '''\n        CALL gds.graph.writeNodeProperties('id-projection',\n            [$property],\n            ['Card', 'Device', 'IP']\n        )\n    ''', params=params)\n    \n    print(f'updating indexes...')\n    run(driver, f'''\n        CREATE INDEX card_{degree_property} IF NOT EXISTS FOR (c:Card) on c.{degree_property};\n    ''')\n    run(driver, f'''\n        CREATE INDEX device_{degree_property} IF NOT EXISTS FOR (c:Device) on c.{degree_property};\n    ''')\n    run(driver, f'''\n        CREATE INDEX ip_{degree_property} IF NOT EXISTS FOR (c:IP) on c.{degree_property};\n    ''')\n    clear_graph(driver, 'id-projection')\n    print('finished.')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Part 1: Exploring Connected Fraud Data"}, {"cell_type": "markdown", "metadata": {}, "source": "### Dataset Introduction\n\nWe will be using an anonymized sample of user accounts and transactions from a real-world Peer-to-Peer (P2P) platform. Prior to ingesting the data into graph, the original identification numbers were removed and categorical values were masked. Each user account has a unique 128-bit identifier, while the other nodes, representing unique credit cards, devices, and ip addresses have been assigned random UUIDs. These identifier are stored as the guid property in the graph schema.\n\nBelow is a visualization of the graph schema:\n\n<img src=\"img/schema.png\" width=\"500\" >\n\nEach user node has an indicator variable for money transfer fraud (named MoneyTransferFraud) that is 1 for known fraud and 0 otherwise. This indicator is determined by a combination of credit card chargeback events and manual review. A chargeback is an action taken by a bank to reverse electronic payments. It involves reversing a payment and triggering a dispute resolution process, often for billing errors and unauthorized credit use. In short, a user must have at least one chargeback to be considered fraudulent. Only a small proportion of the user accounts, roughly 0.7 %, are flagged for fraud.\n\nEach user node has an indicator variable for money transfer fraud (named `MoneyTransferFraud`) that is 1 for known fraud and 0 otherwise. This indicator is determined by a combination of credit card chargeback events and manual review. A chargeback is an action taken by a bank to reverse electronic payments. It involves reversing a payment and triggering a dispute resolution process, often for billing errors and unauthorized credit use. In short, a user must have at least one chargeback to be considered fraudulent. Only a small proportion of the user accounts, roughly 0.7 %, are flagged for fraud."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "\nBelow is a breakdown of high-level counts by labels and relationships as well as the flagged accounts."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# total node counts\nresult = run(driver, 'CALL apoc.meta.stats() YIELD labels AS nodeCounts')\npd.DataFrame(dict(result[0])).transpose()"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# total relationship counts\nresult = run(driver, 'CALL apoc.meta.stats() YIELD relTypesCount as relationshipCounts')\npd.DataFrame(dict(result[0])).transpose()"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, 'MATCH(u:User) RETURN u.fraudMoneyTransfer AS fraudMoneyTransfer, count(u) AS cnt')\npd.DataFrame(dict(record) for record in result)"}, {"cell_type": "markdown", "metadata": {}, "source": "### A Closer Look at Cards and Devices\n\nUpon an initial investigation of aggregate statistics, the labeled fraud accounts do not look well connected which would naturally make one skeptical of a graph based approach.  This is exemplified well by the lack of discrimination in key identifiers.\n\nAs a first step, I break out the ratio of fraud vs non-fraud users connected to credit cards and devices below. I find that very few cards or devices are centered well around flagged accounts. This is a bit surprising for me. If a card or device is used by a flagged account, I would expect the other accounts that use the card/device to also be mostly flagged for fraud as well."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# Setting a label for flagged users will enable faster lookups in cypher and faster gds projections\nrun(driver, 'MATCH(u:User) WHERE u.fraudMoneyTransfer=1 SET u:FlaggedUser RETURN count(u)')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# Use GDS degree centrality to count the number of Users connected to each identifier type - Card, Device, IP\nidentifier_degrees('User', 'degree')\n# Use GDS degree centrality to count the number of FLAGGED Users connected to each identifier type - Card, Device, IP\nidentifier_degrees('FlaggedUser', 'flaggedDegree')\n\n# Calculate the ratio of flagged users to total users\nrun(driver, '''\n        MATCH(n) WHERE n:Card OR n:Device OR n:IP\n        SET n.flaggedRatio = toFloat(n.flaggedDegree)/toFloat(n.degree)\n        RETURN count(n) AS numChanged\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, '''\n    MATCH(n:Card) WHERE n.degree > 1\n    WITH toFloat(count(n)) AS total\n    MATCH(n:Card) WHERE n.degree > 1\n    WITH n, total, CASE\n        WHEN n.flaggedRatio=0 THEN '0'\n        WHEN n.flaggedRatio=1  THEN '1'\n        ELSE 'Between 0-1' END AS flaggedUserRatio\n    RETURN flaggedUserRatio, count(n) AS count, round(toFloat(count(n))/total,3) AS percentCount ORDER BY flaggedUserRatio\n''')\nprint('Flagged User Ratio for Card Count')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, '''\n    MATCH(n:Device) WHERE n.degree > 1\n    WITH toFloat(count(n)) AS total\n    MATCH(n:Device) WHERE n.degree > 1\n    WITH n, total, CASE\n        WHEN n.flaggedRatio=0 THEN '0'\n        WHEN n.flaggedRatio=1  THEN '1'\n        ELSE 'Between 0-1' END AS flaggedUserRatio\n    RETURN flaggedUserRatio, count(n) AS count, round(toFloat(count(n))/total,3) AS percentCount ORDER BY flaggedUserRatio\n''')\nprint('Flagged User Ratio for Device Count')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### Exploring Potential Fraud Patterns with Community Detection\n\nIn this example, it is unclear to me what exactly is taking place, and I have a hunch that the fraud activity is not completely labeled given the lack of connectivity and the limited chargeback logic used to flag fraud. At the same time I do not want to simply label every user that shares a card or device with another flagged account, since it is possible that a benign user's device and or card was used fraudulently by another. Since fraudsters are actively avoiding being identified, actors committing fraud are often not just represented by a single user account, but rather by multiple accounts and identifiers which, hopefully for us, share some connections and similarities.\n\nIn a graph, we can attempt to roughly identify these fragmented identities with Community Detection, a large set of methods that attempt to partition graphs into well connected groups a.k.a. Communities, where the connectivity in the communities is significantly higher than outside the community. There are multiple forms of community detection. We will use a couple in this post, starting with Louvain."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### Using Louvain Community Detection\n\n[Louvain](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/) is useful for exploratory analysis of communities because it uses a form of modularity scoring to split up the graph into hierarchical clusters. This means that your theories around fraud patterns and graph structure don't need to be exact for it to provide informative communities and insights.\n\nI will leverage P2P transactions, cards, and devices for Louvain. We will leave out IP addresses for now since they have some super node issues (we will leverage IP addresses in subsequent parts ). Below are the queries for running Louvain in Neo4j GDS and aggregating community statistics. The last query below orders communities by the count of flagged users so we can further examine some of the more concentrated flagged communities.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "clear_graph(driver, 'comm-projection')\n\nrun(driver, '''\n    CALL gds.graph.project('comm-projection', ['User','Card', 'Device'], {\n        HAS_CC: {orientation: 'UNDIRECTED'},\n        USED: {orientation: 'UNDIRECTED'},\n        P2P: {orientation: 'NATURAL', aggregation: 'SINGLE'}\n        }\n    )\n''')\n\nrun(driver, '''\n    CALL gds.louvain.write('comm-projection', {\n            writeProperty: 'louvainCommunityId'\n    })\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result  = run(driver, '''\n    MATCH (u:User)\n    WITH u.louvainCommunityId AS community,\n        count(u) AS cnt,\n        sum(u.fraudMoneyTransfer) as flaggedCount\n    RETURN community,\n        cnt,\n        flaggedCount,\n        toFloat(flaggedCount)/toFloat(cnt) AS flaggedRatio\n    ORDER BY flaggedCount DESC LIMIT 100\n''')\nprint(\"Louvain Communities Ordered by count of Flagged Users\")\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "We can view communities of users and connecting identifiers in [Neo4j Bloom](https://neo4j.com/docs/bloom-user-guide/current/) or Neo4j Browser with queries of the form:\n\n```cypher\nMATCH(u1:User{louvainCommunityId: $id})-[r1:HAS_CC|HAS_IP|USED]->(n)<-[r2:HAS_CC|HAS_IP|USED]-(u2:User{louvainCommunityId: $id})\nWITH *\nOPTIONAL MATCH (u1)-[:P2P]-(u2)\nRETURN *\n```\nBelow are a couple examples of these communities. Flagged users are colored red with caption=0 with other users colored orange and caption=1. We will see an interesting pattern of flagged users sending money to non-flagged users with which they share devices, cards, and IPs. This behavior is suspicious because it suggests that the same individual or group of individuals may be using multiple accounts to withdraw cash from credit transactions prior to a chargeback being executed on the cards. The shared credit card examples (such as example 2 below) are some of the most indicative.\n\nExample 1:\n<img src=\"img/louvain-community-11.png\" width=\"900\" >\n\nExample 2:\n<img src=\"img/louvain-community-12.png\" width=\"900\" >"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "This pattern indicates that the receiving user accounts are fraud risk accounts. In the next section we will use these patterns to inform resolution of fraud communities and the labeling of additional fraud risk accounts."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Part 2: Resolving Fraud Communities using Entity Resolution and Community Detection\n\nIdentifying communities that reflect underlying groups of individuals is often a key step to fraud detection.  We already explored with Louvain; in this section, in this part, we will provide more formal definitions for resolving entities that will allow us to partition well defined communities in a scalable manner.\n\nTo accomplish this, we will define some Entity Resolution (ER) rules that will allow us to draw relationships between users which we believe belong to the same underlying community. We will then use the [Weakly Connected Components (WCC)](https://neo4j.com/docs/graph-data-science/current/algorithms/wcc/) algorithm to resolve the communities. Lastly, we will label all users in communities that include flagged accounts as fraud risks."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Entity Resolution Business Rules\n\nWe will now use Entity Resolution (ER) to resolve groups of individuals behind sets of user accounts. For this analysis, we will use some pretty straightforward ER business logic. If either of the two below conditions are true, we will resolve two user accounts by linking them together with a new relationship type.\n\n1. One user sent money to another user that shares the same credit card\n2. Two users share a card or device connected to less than or equal to 10 total accounts, and those two users also share at least two other identifiers of type credit card, device, or IP address\n\nYou could switch out or add different rules to the above, these are just examples. In a real-world scenario these business rules would pass by SMEs and possibly be backed by further supervised machine learning on manually labeled data. More advanced techniques for this type of ER are possible in graph and we describe them in [this whitepaper](https://neo4j.com/whitepapers/graph-data-science-use-cases-entity-resolution/) and [this blog](https://neo4j.com/developer-blog/exploring-supervised-entity-resolution-in-neo4j/).\n\nFor a P2P dataset, we do not necessarily want to label all senders/receivers of flagged user transactions as fraudulent since some fraud schemes involve transactions with victims. Furthermore, additional identifiers such as IP may be inexact and cards + devices can be fraudulently controlled/used without the owners permission. Hence I used somewhat stringent rules that aligned with the patterns noted in part 1. We can apply relationships to reflect these business rules using cypher:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# P2P with shared card rule\nrun(driver, '''\n    MATCH (u1:User)-[r:P2P]->(u2)\n    WITH u1, u2, count(r) AS cnt\n    MATCH (u1)-[:HAS_CC]->(n)<-[:HAS_CC]-(u2)\n    WITH u1, u2, count(DISTINCT n) AS cnt\n    MERGE(u1)-[s:P2P_WITH_SHARED_CARD]->(u2)\n    RETURN count(DISTINCT s) AS cnt\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# Shared ids rule\nrun(driver, '''\n    MATCH (u1:User)-[:HAS_CC|USED]->(n)<-[:HAS_CC|USED]-(u2)\n    WHERE n.degree <= 10 AND id(u1) < id(u2)\n    WITH u1, u2, count(DISTINCT n) as cnt\n    MATCH (u1)-[:HAS_CC|USED|HAS_IP]->(m)<-[:HAS_CC|USED|HAS_IP]-(u2)\n    WITH u1, u2, count(DISTINCT m) as cnt\n    WHERE cnt > 2\n    MERGE(u1)-[s:SHARED_IDS]->(u2)\n    RETURN count(DISTINCT s)\n''')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Using Weakly Connected Components (WCC) to Resolve Communities\n\n[Weakly Connected Components (WCC)](https://neo4j.com/docs/graph-data-science/current/algorithms/wcc/) is a practical and highly scalable community detection algorithm. It is also deterministic and very explainable. It defines a community simply as a set of nodes connected by a subset of relationship types in the graph. This makes WCC a good choice for formal community assignment in production fraud detection settings.\n\nBelow we run WCC on users via the ER relationships created above:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "clear_graph(driver, 'comm-projection')\nrun(driver, '''\n    CALL gds.graph.project('comm-projection',\n        ['User'],\n        {\n            SHARED_IDS: {orientation: 'UNDIRECTED'},\n            P2P_WITH_SHARED_CARD: {orientation: 'UNDIRECTED'}\n        }\n    )\n''')\n\nrun(driver, '''\n    CALL gds.wcc.write('comm-projection', {\n        writeProperty: 'wccId'\n    }) YIELD writeMillis,\n        nodePropertiesWritten,\n        componentCount,\n        componentDistribution\n''')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Labeling Fraud Risk User Accounts\n\nAs these communities are meant to label underlying groups of individuals, if even one flagged account is in the community, we will label all user accounts in the group as fraud risks:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    MATCH (f:FlaggedUser)\n    WITH collect(DISTINCT f.wccId) AS flaggedCommunities\n    MATCH(u:User) WHERE u.wccId IN flaggedCommunities\n    SET u:FraudRiskUser\n    SET u.fraudRisk=1\n    RETURN count(u)\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    MATCH (u:User) WHERE NOT u:FraudRiskUser\n    SET u.fraudRisk=0\n    RETURN count(u)\n''')"}, {"cell_type": "markdown", "metadata": {}, "source": "### WCC Community Statistics\n\nThe breakdown of communities by size is listed below. The majority are single user communities. Only a small portion have multiple users and of those, community sizes are mostly 2 and 3. Larger communities are rare. However, if we look at the fraudUser accounts we will see that the majority reside in multi-user communities. The 118 fraud accounts in single user communities are flagged users (via original chargeback logic) that have yet to be resolved to a community."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, '''\n    MATCH (u:User)\n    WITH u.wccId AS community, count(u) AS cSize, sum(u.fraudRisk) AS cFraudSize\n    WITH community, cSize, cFraudSize,\n    CASE\n        WHEN cSize=1 THEN ' 1'\n        WHEN cSize=2 THEN ' 2'\n        WHEN cSize=3 THEN ' 3'\n        WHEN cSize>3 AND cSize<=10 THEN ' 4-10'\n        WHEN cSize>10 AND cSize<=50 THEN '11-50'\n        WHEN cSize>10 THEN '>50' END AS componentSize\n    RETURN componentSize, count(*) as numberOfComponents, sum(cSize) AS totalUserCount, sum(cFraudSize) AS fraudUserCount ORDER BY componentSize\n''')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Similar to Louvain we can view communities of users and connecting identifiers in [Neo4j Bloom](https://neo4j.com/docs/bloom-user-guide/current/) or Browser with queries of the form:\n\n```cypher\nMATCH(u1:User{wccId: $id})-[r1:HAS_CC|HAS_IP|USED]->(n)<-[r2:HAS_CC|HAS_IP|USED]-(u2:User{wccId: $id})\nWITH *\nOPTIONAL MATCH (u1)-[r3:P2P]-(u2)\nRETURN *\n```\nBelow are a couple examples. Users that were flagged via initial chargeback logic are colored red with caption=1, while other users are colored orange with caption=0. Overall, you will notice a high degree of overlapping connectivity of identifiers and P2P transactions between users, which we should expect given our ER rules.\n\nExample 1:\n<img src=\"img/wcc-community-12.png\" width=\"900\" >\n\nExample 2:\n<img src=\"img/wcc-community-11.png\" width=\"900\" >"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Outcomes of Fraud Risk Labeling\nFraud Risk labeling helped identify an additional 211 new fraud risk user accounts, nearly doubling the number of known fraud users (87.5% increase). We also see that 65% of the money going to/from previously flagged accounts and other users can be attributed to the newly identified risk accounts:"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n   MATCH (:FlaggedUser)-[r:P2P]-(u)  WHERE NOT u:FlaggedUser\n   WITH toFloat(sum(r.totalAmount)) AS p2pTotal\n   MATCH (u:FraudRiskUser)-[r:P2P]-(:FlaggedUser) WHERE NOT u:FlaggedUser\n   WITH p2pTotal,  toFloat(sum(r.totalAmount)) AS fraudRiskP2pTotal\n   RETURN round((fraudRiskP2pTotal)/p2pTotal,3) AS p\n''')[0].get('p')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Additionally, while the newly identified 211 accounts represents less than 1% of total users in the sample, 12.7% of the total P2P amount in the sample involved the newly identified accounts as senders or receivers:"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n   MATCH (:User)-[r:P2P]->()\n   WITH toFloat(sum(r.totalAmount)) AS p2pTotal\n   MATCH (u:FraudRiskUser)-[r:P2P]-() WHERE NOT u:FlaggedUser\n   WITH p2pTotal, toFloat(sum(r.totalAmount)) AS fraudRiskP2pTotal\n   RETURN round((fraudRiskP2pTotal)/p2pTotal,3) AS p\n''')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Finally, we can see an improvement in card and device discrimination with many more cards and devices being used by fraud risk accounts exclusively."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "identifier_degrees('FraudRiskUser', 'fraudRiskDegree')\n\nrun(driver, '''\n        MATCH(n) WHERE n:Card OR n:Device OR n:IP\n        SET n.fraudRiskRatio = toFloat(n.fraudRiskDegree)/toFloat(n.degree)\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, '''\n    MATCH(n:Card) WHERE n.degree > 1\n    WITH toFloat(count(n)) AS total\n    MATCH(n:Card) WHERE n.degree > 1\n    WITH n, total, CASE\n        WHEN n.fraudRiskRatio=0 THEN '0'\n        WHEN n.fraudRiskRatio=1  THEN '1'\n        ELSE 'Between 0-1' END AS fraudRiskRatio\n    RETURN fraudRiskRatio, count(n) AS count, round(toFloat(count(n))/total,3) AS percentCount ORDER BY fraudRiskRatio\n''')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, '''\n    MATCH(n:Device) WHERE n.degree > 1\n    WITH toFloat(count(n)) AS total\n    MATCH(n:Device) WHERE n.degree > 1\n    WITH n, total, CASE\n        WHEN n.fraudRiskRatio=0 THEN '0'\n        WHEN n.fraudRiskRatio=1  THEN '1'\n        ELSE 'Between 0-1' END AS fraudRiskRatio\n    RETURN fraudRiskRatio, count(n) AS count, round(toFloat(count(n))/total,3) AS percentCount ORDER BY fraudRiskRatio\n''')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "The aggregate P2P statistics combined with improvements in Card and Device metrics are significant given the limited scope of the previously flagged fraud which focused on chargebacks.  These results strongly imply that there are more sophisticated networks of fraudulent money flows behind the chargebacks rather than the chargebacks being isolated occurrences."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Part 3: Recommending Suspicious Accounts With Centrality & Node Similarity\n\nIn parts 1 & 2 we explored the graph and identified high risk fraud communities. At this stage, we may want to expand beyond our business logic to automatically identify other users that are suspiciously similar to the fraud risks already identified. Neo4j and GDS makes it simple to triage and recommend such suspect users in a matter of seconds. We can leverage both centrality and similarity algorithms for this.\n"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Using Weighted Degree Centrality to Recommend Potential High Risk Accounts\n\nWe can quickly and easily generate a ranked list of suspicious user accounts with weighted degree centrality. Specifically, we can calculate the degree centrality of users in respect to their identifiers (Devices, Cards, and IPs) weighted by the fraudRiskRatios we made in part 2. In this case, a simple Cypher query suffices.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver,'''\n    MATCH(f:FraudRiskUser)-[:HAS_CC|HAS_IP|USED]->(n)\n    WITH DISTINCT n\n    MATCH(u:User)-[:HAS_CC|HAS_IP|USED]->(n) WHERE NOT u:FraudRiskUser\n    WITH left(u.guid,8) as uid,\n        sum(n.fraudRiskRatio) AS totalIdFraudRisk,\n        count(n) AS numberFraudRiskIds\n    WITH uid, totalIdFraudRisk,\n        numberFraudRiskIds,\n        totalIdFraudRisk/toFloat(numberFraudRiskIds) AS averageFraudIdRisk\n    WHERE averageFraudIdRisk >= 0.25\n    RETURN uid, totalIdFraudRisk, numberFraudRiskIds, averageFraudIdRisk\n    ORDER BY totalIdFraudRisk DESC LIMIT 10\n''')\n\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Users in the above result list are sorted by how much identifying information they share with previously labeled fraud risks, the ones with the most being at the top. Technically speaking, these users are ranked by their total Id fraud risk, which is equal to the sum of the fraudRiskRatios from the Identifiers they are connected to. In the query I also implement a limit on the average fraud risk to avoid users that just have a lot of high-degree identifiers (likely proxy ip addresses shared by only a small fraction of fraud risk users). This sort of filtering can be tweaked by use case to get the right balance between total risk vs average risk.\n\nIn a real-world fraud detection use case, these results can be triaged by analysts to label more fraud accounts and grow labeled fraud communities."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Using Node Similarity to Expand on Fraud Communities\n\nSimple calculations like weighted degree centrality work well for identifying suspicious users over the whole graph, but what if we are interested in how users are related to a specific fraud risk community or set of communities? Perhaps we hypothesize that communities of fraud risk users are actually bigger than currently represented but we don't have exact business rules to apply. We can leverage similarity algorithms to help us score and recommend users for this.\n\nGDS offers multiple algorithms for similarity. In this analysis I will focus on the aptly named  Node Similarity algorithm \u2013 you can read more about it [here](https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/). Node similarity parallelizes well and is explainable. It identifies pairs of similar nodes based on a straightforward Jaccard similarity calculation. So while other ML-based similarity approaches like FastRP + KNN covered [in this post](https://neo4j.com/developer-blog/exploring-practical-recommendation-systems-in-neo4j/) scale well for running globally on very large graphs, Node Similarity is a good choice where explainability is important and you can narrow down the universe of comparisons to a subset of your data. Examples of narrowing down include focusing on just single communities, newly added users, or users within a specific proximity to suspect accounts. In this analysis we will take the third approach, filtering to just those Cards, Devices, and IP addresses that connect to at least one fraud risk account from part 2.\n\n\nBelow I apply three queries to calculate node similarity. The first query enables the identifier filtering via setting a new label on the Card, Device, and IP address nodes that connect to fraud risk accounts. The first query also weights relationships by the inverse of degree centrality, essentially downplaying the importance of identifiers proportional to the number of other users they connect to. This is important as some identifiers, particularly IP addresses, can connect to hundreds or thousands of users, in which case the identifier may be very generic (like a proxy IP address) and not as relevant to true user identity. The second and third query project the graph and write relationships back to the database with a score to represent similarity strength between user node pairs. You will notice that I use a similarity cutoff of 0.01 in the third query, which is intended to rule out weak associations and keep the similarities relevant.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# label identifiers and users that are close to fraud risk users and assign inverse degree weight\nrun(driver,'''\n    MATCH(f:FraudRiskUser)-[:HAS_CC|HAS_IP|USED]->(n)\n    WITH DISTINCT n\n    MATCH(n)<-[r:HAS_CC|HAS_IP|USED]-(u)\n    SET n:FraudSharedId\n    SET r.inverseDegreeWeight = 1.0/(n.degree-1.0)\n    RETURN count(DISTINCT n)\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "clear_graph(driver, 'similarity-projection')\nrun(driver, '''\n    CALL gds.graph.project('similarity-projection',\n        ['User', 'FraudSharedId'],\n        ['HAS_CC', 'USED', 'HAS_IP'],\n        {relationshipProperties: ['inverseDegreeWeight']}\n    ) YIELD nodeCount, relationshipCount\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    CALL gds.nodeSimilarity.write('similarity-projection',{\n        writeRelationshipType: 'SIMILAR_IDS',\n        writeProperty: 'score',\n        similarityCutoff: 0.01,\n        relationshipWeightProperty: 'inverseDegreeWeight'\n    }) YIELD relationshipsWritten, similarityDistribution\n''')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "From there, we can run a Cypher query to rank users by how similar they are to known fraud risk communities."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "#get nodes similar to the high risk ones\nresult = run(driver, '''\n    MATCH (f:FraudRiskUser)\n    WITH f.wccId AS componentId, count(*) AS numberOfUsers, collect(f) AS users\n    UNWIND users AS f\n    MATCH (f)-[s:SIMILAR_IDS]->(u:User) WHERE NOT u:FraudRiskUser AND numberOfUsers > 2\n    RETURN u.guid AS userId, sum(s.score) AS totalScore, collect(DISTINCT componentId) AS closeToCommunityIds ORDER BY totalScore DESC\n''')\npd.DataFrame([dict(record) for record in result])"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Let\u2019s take a look at the first user in the list, user `0b3f278f`, in [Bloom](https://neo4j.com/docs/bloom-user-guide/current/) . This user account seems interesting in the sense that they connect to two different communities.\n\n<img src=\"img/node-similarity-1.png\" width=\"700\" >\n\nYou can see how this user connects to the two fraud risk communities and how the similarity relationships were based on shared IP addresses. This user seems to act as a sort of bridge between the two communities, suggesting not only that the user is likely part of the fraud communities but also that the two communities may actually reflect one-in-the same."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "\nOverall, centrality and similarity metrics like these are fast and easy to implement with Neo4j and GDS. They can help advance your Data Science approach by introducing automated and semi-supervised processes to assist in targeted triage and identification of suspicious user accounts based on previously labeled data."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Part 4: Predicting Fraud Risk Accounts with Machine Learning\n\nIn real-world scenarios we often don't know which user accounts are fraudulent ahead of time. There will be cases, like with this dataset, where some accounts get flagged due to business rules, for example chargeback history, or via user reporting mechanisms. However, as we saw in parts 1 & 2 above, those flags don't tell the whole story. The community detection and recommendation approaches we previously covered can go a very long way in helping us understand this story and label additional fraud risk users and patterns. That said, there are multiple reasons why we may want to add supervised Machine Learning to predict the additional fraud risks:\n\n - __Proactive Detection:__ We can train a model to identify fraudulent actors ahead of time (such as before additional chargebacks or system flags) and better identify new communities that aren't connected to older known fraud accounts.\n - __Measurable Performance:__ Supervised learning models produce clear performance metrics that enable us to evaluate and adjust as needed\n - __Automation:__ supervised Machine Learning automates the prediction of fraud risk accounts.\n\nIn the below sections we will walk through how to engineer graph features for ML, export those features to python, then train and evaluate an ML model for fraud classification."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Feature Engineering\nIf we want a machine learning model to successfully classify fraud risk user accounts, we need to supply features that will be informative for the task. The below commands engineer graph features using GDS.  This includes features from [WCC](https://neo4j.com/docs/graph-data-science/current/algorithms/wcc/) community sizes, [pageRank](https://neo4j.com/docs/graph-data-science/current/algorithms/page-rank/), and [degree centrality](https://neo4j.com/docs/graph-data-science/current/algorithms/degree-centrality/)."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### Community Features"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    MATCH (u:User)\n    WITH u.wccId AS componentId, count(*) AS communitySize, collect(u) AS users\n    WITH communitySize, toInteger(communitySize > 1) AS partOfCommunity, users\n    UNWIND users as u\n    SET u.communitySize = communitySize\n    SET u.partOfCommunity = partOfCommunity\n    RETURN count(u)\n''')"}, {"cell_type": "markdown", "metadata": {}, "source": "#### P2P With Cards and Id Sharing Centrality Features"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "clear_graph(driver,'user-feature-projection')\nrun(driver, '''\n    CALL gds.graph.project('user-feature-projection', ['User'], {\n        P2P: {type: 'P2P', orientation: 'NATURAL', aggregation: 'SUM', properties: ['totalAmount']},\n        P2P_REVERSE: {type: 'P2P', orientation: 'REVERSE', aggregation: 'SUM', properties: ['totalAmount']},\n        SHARED_IDS: {type: 'SHARED_IDS', orientation: 'UNDIRECTED'},\n        P2P_WITH_SHARED_CARD: {type: 'P2P_WITH_SHARED_CARD', orientation: 'NATURAL'},\n        P2P_WITH_SHARED_CARD_REVERSE: {type: 'P2P_WITH_SHARED_CARD', orientation: 'REVERSE'}\n    })\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['SHARED_IDS'],\n        writeProperty: 'sharedIdsDegree'\n    })\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P_WITH_SHARED_CARD'],\n        maxIterations: 1000,\n        writeProperty: 'p2pSharedCardPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P_WITH_SHARED_CARD'],\n        writeProperty: 'p2pSharedCardDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P_WITH_SHARED_CARD_REVERSE'],\n        maxIterations: 1000,\n        writeProperty: 'p2pReversedSharedCardPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P_WITH_SHARED_CARD_REVERSE'],\n        writeProperty: 'p2pReversedSharedCardDegree'\n    })\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P'],\n        maxIterations: 1000,\n        writeProperty: 'p2pSentPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P'],\n        maxIterations: 1000,\n        relationshipWeightProperty: 'totalAmount',\n        writeProperty: 'p2pSentWeightedPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P'],\n        relationshipWeightProperty: 'totalAmount',\n        writeProperty: 'p2pSentWeightedDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P'],\n        writeProperty: 'p2pSentDegree'\n    })\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "run(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P_REVERSE'],\n        maxIterations: 1000,\n        writeProperty: 'p2pReceivedPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.pageRank.write('user-feature-projection', {\n        relationshipTypes: ['P2P_REVERSE'],\n        maxIterations: 1000,\n        relationshipWeightProperty: 'totalAmount',\n        writeProperty: 'p2pReceivedWeightedPageRank'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P_REVERSE'],\n        relationshipWeightProperty: 'totalAmount',\n        writeProperty: 'p2pReceivedWeightedDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.write('user-feature-projection', {\n        relationshipTypes: ['P2P_REVERSE'],\n        writeProperty: 'p2pReceivedDegree'\n    })\n''')\nclear_graph(driver, 'user-feature-projection')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### ID Centrality Features"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "clear_graph(driver, 'user-id-degree-projection')\nrun(driver, '''\n    CALL gds.graph.create('user-id-degree-projection',\n        ['User', 'Card', 'Device', 'IP'],\n        ['HAS_CC', 'HAS_IP', 'USED']\n    )\n''')\n\nrun(driver, '''\n    CALL gds.degree.mutate('user-id-degree-projection', {\n        nodeLabels: ['User', 'Card'],\n        relationshipTypes: ['HAS_CC'],\n        mutateProperty: 'cardDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.mutate('user-id-degree-projection', {\n        nodeLabels: ['User', 'Device'],\n        relationshipTypes: ['USED'],\n        mutateProperty: 'deviceDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.degree.mutate('user-id-degree-projection', {\n        nodeLabels: ['User', 'IP'],\n        relationshipTypes: ['HAS_IP'],\n        mutateProperty: 'ipDegree'\n    })\n''')\n\nrun(driver, '''\n    CALL gds.graph.writeNodeProperties('user-id-degree-projection',\n        ['cardDegree', 'deviceDegree', 'ipDegree'],\n        ['User']\n    )\n''')\nclear_graph(driver, 'user-id-degree-projection')"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Machine Learning Training & Evaluation"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### Get and Prepare Data"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "%%time\nresult = run(driver, 'MATCH(u:User) RETURN u')\ndf = pd.DataFrame([dict(record.get('u')) for record in result])\ndf"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "result = run(driver, 'MATCH(u:User) RETURN u')\ndf = pd.DataFrame([dict(record.get('u')) for record in result])\n\nX = df[[\n    'sharedIdsDegree',\n    'p2pSharedCardPageRank',\n    'p2pSentPageRank',\n    'p2pReceivedWeightedPageRank',\n    'p2pReceivedWeightedDegree',\n    'ipDegree',\n    'cardDegree',\n    'deviceDegree',\n    'communitySize',\n    'partOfCommunity'\n]]\n\ny = df.fraudRisk - df.fraudMoneyTransfer"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "X"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "print(y)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "#### Model Training and Evaluation\n\nFor purposes of this demo I am going to use a random forest classifier. Other classifiers including logistic regression, SVM, Neural Nets and Boosting variants could work as well. Going into the exact pros and cons of these models is out of scope here. Overall, I like exploring classification with Random Forests since they are relatively robust to feature scaling and collinearity issues and require minimal tuning to get working well."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=500, random_state=0, max_depth=5, bootstrap=True, class_weight='balanced')\nclf.fit(X_train, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.metrics import ConfusionMatrixDisplay\nprint('Accuracy of random forrest classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\nConfusion Matrix: ')\ndisp = ConfusionMatrixDisplay.from_predictions(y_test, clf.predict(X_test), display_labels=clf.classes_,\n                                               normalize='true', cmap='Greys', colorbar=False)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.metrics import RocCurveDisplay\n\ndisplay = RocCurveDisplay.from_estimator(clf, X_test, y_test, name=\"RF Model\")\n_ = display.ax_.set_title(\"ROC Curve\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.metrics import PrecisionRecallDisplay\n\ny_prob = clf.predict_proba(X_test)\ndisplay = PrecisionRecallDisplay.from_predictions(y_test, y_prob[:, 1], name=\"RF Model\")\n_ = display.ax_.set_title(\"Precision-Recall Curve\")"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Below is a ranked list of the most influential features. Among the most important are the community sizes and the shared ids degree and p2p shared card pageRank."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "from sklearn.inspection import permutation_importance\nresult = permutation_importance(clf, X_train, y_train, random_state=0)\npd.DataFrame(abs(result['importances_mean']),index=X_train.columns).sort_values(0, ascending=False)"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "### Investigating Unlabeled High-Probability Fraud Risk Predictions\nThe labeling from part 2 wasn't perfect. Now that we have trained a machine learning model, investigating user accounts that were predicted as high probability fraud risks despite not being labeled as such by us (ostensible false positives), will bring further insights.\n\nThe below commands will isolate some cases from the test set so we can visualize in [Neo4j Bloom](https://neo4j.com/product/bloom/)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# Retrieve High Probability predictions for non-fraud risk labeled data in the testset\ny_test_df = y_test.to_frame(name='cls')\ny_test_df['predictedProbability']=y_prob[:, 1]\ntest_prob_df = y_test_df[(y_test_df.predictedProbability > 0.88) & (y_test_df.cls == 0)].join(df[['guid','wccId', 'communitySize']])\ntest_prob_df"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "#Write back to database for investigation in Bloom\nfor index, row in test_prob_df.iterrows():\n    run(driver, '''\n        MATCH(u:User) WHERE u.guid = $guid\n        SET u.predictedProbability = $predictedProbability\n    ''', params = row.to_dict())"}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "Below are a couple examples from the test set visualized in [Bloom](https://neo4j.com/docs/bloom-user-guide/current/) . The larger, leftmost, green nodes with user icons represent the high probability nodes of interest.\n\nExample 1\n<img src=\"img/predictions-in-bloom-1.png\" width=\"700\" >\\n\n\nExample 2\n<img src=\"img/predictions-in-bloom-2.png\" width=\"700\" >\n\n\nPerhaps unsurprisingly, they exhibit the P2P with shared card behavior, they also have a relatively large number of credit cards (the median degree centrality on cards is 3). This could potentially be a sign of fraud, though it is hard to know on an anonymized dataset like this. This is where subject matter expert review and iteration comes in. If this behavior turns out to be a clear indicator of fraud, it means we are predicting fraud more proactively before chargebacks take place which is the ideal. In this case, if we re-label these users appropriately and re-train our ML model as more data comes in, we will further improve predictive performance. If, on the other hand, it turns out that some of this behavior is benign, we can adjust the feature engineer and model so the ML learns to rule out such cases which will likewise improve predictive performance and increase our understanding of fraud patterns. Either way, it is a win."}, {"cell_type": "markdown", "metadata": {"pycharm": {"name": "#%% md\n"}}, "source": "## Clean Up\nThis section will help clean all the additional graph elements and properties created in the above workflow."}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# delete created relationships\nrun(driver,'MATCH (:User)-[r:SHARED_IDS]->() DELETE r')\nrun(driver,'MATCH (:User)-[r:P2P_WITH_SHARED_CARD]->() DELETE r')\nrun(driver,'MATCH (:User)-[r:SIMILAR_IDS]->() DELETE r')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# remove created node Labels\nrun(driver,'MATCH (u:FlaggedUser) REMOVE u:FlaggedUser')\nrun(driver,'MATCH (u:FraudRiskUser) REMOVE u:FraudRiskUser')\nrun(driver,'MATCH (u:FraudSharedId) REMOVE u:FraudSharedId')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# remove created node properties\nrun(driver,'''\n    MATCH (n)\n    REMOVE n.wccId,\n        n.sharedIdsDegree,\n        n.predictedProbability,\n        n.partOfCommunity,\n        n.p2pSharedCardPageRank,\n        n.p2pSharedCardDegree,\n        n.p2pSentWeightedPageRank,\n        n.p2pSentWeightedDegree,\n        n.p2pSentPageRank,\n        n.p2pSentDegree,\n        n.p2pReversedSharedCardPageRank,\n        n.p2pReversedSharedCardDegree,\n        n.p2pReceivedWeightedPageRank,\n        n.p2pReceivedWeightedDegree,\n        n.p2pReceivedPageRank,\n        n.p2pReceivedDegree,\n        n.louvainCommunityId,\n        n.ipDegree,\n        n.fraudRiskRatio,\n        n.fraudRiskDegree,\n        n.fraudRisk,\n        n.flaggedRatio,\n        n.flaggedDegree,\n        n.deviceDegree,\n        n.degree,\n        n.communitySize,\n        n.cardDegree\n''')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# remove created relationship properties\nrun(driver, 'MATCH ()-[r]->() REMOVE r.inverseDegreeWeight')"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": "# Remove GDS graph projections\nclear_all_graphs(driver)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false, "jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}